{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: HTTP Error 429: Too Many Requests\n",
      "[nltk_data] Error loading punkt: HTTP Error 429: Too Many Requests\n",
      "[nltk_data] Error loading omw-1.4: HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(filepath):\n",
    "    df = read_csv(filepath, encoding='cp1252')\n",
    "    references = df[\"True Title\"]\n",
    "    predictions = df[\"Predicted Title\"]\n",
    "    print(rouge.compute(predictions=predictions,references=references))\n",
    "    print(bleu.compute(predictions=predictions,references=[[ref] for ref in references]))\n",
    "    print(meteor.compute(predictions=predictions,references=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10182\n",
      "10182\n",
      "{'rouge1': 0.1366115222470731, 'rouge2': 0.04254364167446087, 'rougeL': 0.13179486331567025, 'rougeLsum': 0.1318208908662652}\n",
      "{'bleu': 0.036445868433343596, 'precisions': [0.1688180085410283, 0.06265792419638573, 0.034361192065431985, 0.03463917525773196], 'brevity_penalty': 0.6118439469006668, 'length_ratio': 0.6705657752107298, 'translation_length': 41447, 'reference_length': 61809}\n",
      "{'meteor': 0.09324901248878967}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9838\n",
      "9838\n",
      "{'rouge1': 0.15158954967586952, 'rouge2': 0.04889780780488909, 'rougeL': 0.14419852127153018, 'rougeLsum': 0.14434043878556196}\n",
      "{'bleu': 0.036447028298634054, 'precisions': [0.15408003706492152, 0.04847205807867635, 0.023869779118498573, 0.01753720971369368], 'brevity_penalty': 0.8667631546259996, 'length_ratio': 0.8748986623429267, 'translation_length': 69068, 'reference_length': 78944}\n",
      "{'meteor': 0.11078987532467596}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_attention_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1429085850744704, 'rouge2': 0.04106810454799074, 'rougeL': 0.13470933453489325, 'rougeLsum': 0.1348892319191301}\n",
      "{'bleu': 0.03166583165349626, 'precisions': [0.14564057307538505, 0.04218493030385285, 0.021559815689174585, 0.016730836541827093], 'brevity_penalty': 0.8207110764935666, 'length_ratio': 0.835014392630973, 'translation_length': 72521, 'reference_length': 86850}\n",
      "{'meteor': 0.09750387387678204}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_pointer_generator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1535061909525322, 'rouge2': 0.04740998192119289, 'rougeL': 0.14479962254003736, 'rougeLsum': 0.14479650082217893}\n",
      "{'bleu': 0.03548716771466826, 'precisions': [0.15497928703804645, 0.0461686792691408, 0.02268793739182783, 0.016938156034942187], 'brevity_penalty': 0.8714666574092829, 'length_ratio': 0.8790608528988979, 'translation_length': 73384, 'reference_length': 83480}\n",
      "{'meteor': 0.10596625515984237}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_vanilla_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import openai\n",
    "import random\n",
    "\n",
    "\n",
    "first_as_token_id = 3983\n",
    "second_as_token_id = 5686\n",
    "\n",
    "instruction = \"\"\"\n",
    "    You are given an abstract of a published real world news and a pair of possible titles. \n",
    "    One of the titles is the official title used by the news agency, which is generated \n",
    "    by a professional human. The other one is generated by a machine learning algorithm \n",
    "    and can be incorrect, incomplete, vague, or sound unprofessional or unnatural. \n",
    "    Your job is to identify whether the first or the second title \n",
    "    is the original human-generated title. \n",
    "    \n",
    "    Answer with a single word: \"first\" or \"second\", nothing else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result_files = [\n",
    "    \"LSTM_attention_stopwords.csv\",\n",
    "    \"LSTM_attention.csv\",\n",
    "    \"LSTM_pointer_generator.csv\",\n",
    "    \"LSTM_vanilla_data.csv\"\n",
    "]\n",
    "\n",
    "def generate_discriminator_training_data():\n",
    "    with open(\"discriminator_examples.jsonl\", \"w\") as f:\n",
    "        for filename in result_files:\n",
    "            df = read_csv(filename, encoding='cp1252')\n",
    "            df = df.sample(n=5)\n",
    "            \n",
    "            for item in df.iterrows():\n",
    "                item=item[1]\n",
    "                \n",
    "                abstract = item[\"Abstract\"]\n",
    "                reference = item[\"True Title\"]\n",
    "                prediction = item[\"Predicted Title\"]\n",
    "                \n",
    "                correct_answer = random.choice([\"first\",\"second\"])\n",
    "                if correct_answer == \"first\":\n",
    "                    first = reference\n",
    "                    second = prediction\n",
    "                    incorrect_answer = \"second\"\n",
    "                elif correct_answer == \"second\":\n",
    "                    first = prediction\n",
    "                    second = reference\n",
    "                    incorrect_answer = \"first\"\n",
    "                \n",
    "                prompt = f\"\"\"\n",
    "                    Abstract: {abstract}\n",
    "                    First title: {first}\n",
    "                    Second title: {second}\n",
    "                \"\"\"\n",
    "            \n",
    "                training_sample = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": instruction},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                        {\"role\": \"assistant\", \"content\": correct_answer}\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                training_json = json.dumps(training_sample)\n",
    "                f.write(training_json+\"\\n\")\n",
    "\n",
    "\n",
    "generate_discriminator_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "class OpenAIFineTuner:\n",
    "    \"\"\"\n",
    "    Class to fine tune OpenAI models\n",
    "    \"\"\"\n",
    "    def __init__(self, training_file_path, model_name, suffix):\n",
    "        self.training_file_path = training_file_path\n",
    "        self.model_name = model_name\n",
    "        self.suffix = suffix\n",
    "        self.file_object = None\n",
    "        self.fine_tuning_job = None\n",
    "        self.model_id = None\n",
    "\n",
    "    def create_openai_file(self):\n",
    "        self.file_object = client.files.create(\n",
    "            file=open(self.training_file_path, \"rb\"),\n",
    "            purpose=\"fine-tune\",\n",
    "        )\n",
    "\n",
    "    def wait_for_file_processing(self, sleep_time=20):\n",
    "        print(self.file_object.status)\n",
    "        while self.file_object.status != 'processed':\n",
    "            time.sleep(sleep_time)\n",
    "            print(\"File Status: \", self.file_object.status)\n",
    "\n",
    "    def create_fine_tuning_job(self):\n",
    "        self.fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "            training_file=self.file_object.id,\n",
    "            model=self.model_name,\n",
    "            suffix=self.suffix,\n",
    "        )\n",
    "\n",
    "    def wait_for_fine_tuning(self, sleep_time=45):\n",
    "        while client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).status != 'succeeded':\n",
    "            time.sleep(sleep_time)\n",
    "            print(\"Job Status: \", client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).status)\n",
    "\n",
    "    def retrieve_fine_tuned_model(self):\n",
    "        self.model_id = client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).fine_tuned_model\n",
    "        return self.model_id\n",
    "\n",
    "    def fine_tune_model(self):\n",
    "        self.create_openai_file()\n",
    "        self.wait_for_file_processing()\n",
    "        self.create_fine_tuning_job()\n",
    "        self.wait_for_fine_tuning()\n",
    "        return self.retrieve_fine_tuned_model()\n",
    "\n",
    "fine_tuner = OpenAIFineTuner(\n",
    "    # put your dumped train.jsonl file here\n",
    "    training_file_path=\"discriminator_examples.jsonl\",\n",
    "    model_name=\"gpt-3.5-turbo-1106\",\n",
    "    suffix=\"discriminator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  succeeded\n",
      "ft:gpt-3.5-turbo-1106:worcester-polytechnic-institute:discriminator:9CX6Fawj\n"
     ]
    }
   ],
   "source": [
    "fine_tuner.fine_tune_model()\n",
    "model_id = fine_tuner.model_id\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminate_title_pair_with_llm(abstract,reference,prediction):\n",
    "    correct_answer = random.choice([\"first\",\"second\"])\n",
    "    if correct_answer == \"first\":\n",
    "        first = reference\n",
    "        second = prediction\n",
    "        incorrect_answer = \"second\"\n",
    "    elif correct_answer == \"second\":\n",
    "        first = prediction\n",
    "        second = reference\n",
    "        incorrect_answer = \"first\"\n",
    "    \n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Abstract: {abstract}\n",
    "        First title: {first}\n",
    "        Second title: {second}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        logit_bias={first_as_token_id: 100, second_as_token_id: 100},\n",
    "        max_tokens=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    model_completion = completion.choices[0].logprobs\n",
    "    json_response = json.loads(model_completion.json())\n",
    "    logprob = json_response[\"content\"][0][\"logprob\"]\n",
    "    token = json_response[\"content\"][0][\"token\"].lower()\n",
    "    probability = math.exp(logprob)\n",
    "\n",
    "\n",
    "    if token == correct_answer:\n",
    "        correct_flag = 1\n",
    "    elif token == incorrect_answer:\n",
    "        correct_flag = 0\n",
    "    else:\n",
    "        raise RuntimeError(f\"{json_response} is unexpected\")\n",
    "    \n",
    "    return correct_flag, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def llm_as_discriminator(filepath, max_infer=100):\n",
    "    df = read_csv(filepath, encoding='cp1252')\n",
    "    total_correct = 0\n",
    "    probabilities = {\n",
    "        \"correct\": [],\n",
    "        \"incorrect\": []\n",
    "    }\n",
    "    for idx in tqdm(range(max_infer)):\n",
    "        item = df.iloc[idx]\n",
    "        abstract = item[\"Abstract\"]\n",
    "        reference = item[\"True Title\"]\n",
    "        prediction = item[\"Predicted Title\"]\n",
    "        correct_flag, probability = discriminate_title_pair_with_llm(abstract,reference,prediction)\n",
    "        total_correct += correct_flag\n",
    "        if correct_flag==1:\n",
    "            probabilities[\"correct\"].append(probability)\n",
    "        elif correct_flag==0:\n",
    "            probabilities[\"incorrect\"].append(probability)\n",
    "        else:\n",
    "            raise RuntimeError(f\"{correct_flag} is unexpected\")\n",
    "    print(f\"Discrimination accuracy: {100*total_correct/max_infer}\")\n",
    "    print(f\"Average confidence of correct discrimination: {np.mean(probabilities['correct'])}\")\n",
    "    print(f\"Average confidence of incorrect discrimination: {np.mean(probabilities['incorrect'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100/100 [00:42<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 91.0\n",
      "Average confidence of correct discrimination: 0.9999954968193302\n",
      "Average confidence of incorrect discrimination: 0.9750113348767333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
