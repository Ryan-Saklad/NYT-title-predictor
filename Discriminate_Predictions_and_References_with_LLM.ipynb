{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:27:56.116750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 18:27:56.662619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(filepath):\n",
    "    df = read_csv(filepath, encoding='cp1252')\n",
    "    references = df[\"True Title\"]\n",
    "    predictions = df[\"Predicted Title\"]\n",
    "    print(rouge.compute(predictions=predictions,references=references))\n",
    "    print(bleu.compute(predictions=predictions,references=[[ref] for ref in references]))\n",
    "    print(meteor.compute(predictions=predictions,references=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10182\n",
      "10182\n",
      "{'rouge1': 0.1366115222470731, 'rouge2': 0.04254364167446087, 'rougeL': 0.13179486331567025, 'rougeLsum': 0.1318208908662652}\n",
      "{'bleu': 0.036445868433343596, 'precisions': [0.1688180085410283, 0.06265792419638573, 0.034361192065431985, 0.03463917525773196], 'brevity_penalty': 0.6118439469006668, 'length_ratio': 0.6705657752107298, 'translation_length': 41447, 'reference_length': 61809}\n",
      "{'meteor': 0.09324901248878967}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9838\n",
      "9838\n",
      "{'rouge1': 0.15158954967586952, 'rouge2': 0.04889780780488909, 'rougeL': 0.14419852127153018, 'rougeLsum': 0.14434043878556196}\n",
      "{'bleu': 0.036447028298634054, 'precisions': [0.15408003706492152, 0.04847205807867635, 0.023869779118498573, 0.01753720971369368], 'brevity_penalty': 0.8667631546259996, 'length_ratio': 0.8748986623429267, 'translation_length': 69068, 'reference_length': 78944}\n",
      "{'meteor': 0.11078987532467596}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_attention_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1429085850744704, 'rouge2': 0.04106810454799074, 'rougeL': 0.13470933453489325, 'rougeLsum': 0.1348892319191301}\n",
      "{'bleu': 0.03166583165349626, 'precisions': [0.14564057307538505, 0.04218493030385285, 0.021559815689174585, 0.016730836541827093], 'brevity_penalty': 0.8207110764935666, 'length_ratio': 0.835014392630973, 'translation_length': 72521, 'reference_length': 86850}\n",
      "{'meteor': 0.09750387387678204}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_pointer_generator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1535061909525322, 'rouge2': 0.04740998192119289, 'rougeL': 0.14479962254003736, 'rougeLsum': 0.14479650082217893}\n",
      "{'bleu': 0.03548716771466826, 'precisions': [0.15497928703804645, 0.0461686792691408, 0.02268793739182783, 0.016938156034942187], 'brevity_penalty': 0.8714666574092829, 'length_ratio': 0.8790608528988979, 'translation_length': 73384, 'reference_length': 83480}\n",
      "{'meteor': 0.10596625515984237}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"LSTM_vanilla_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import openai\n",
    "import random\n",
    "\n",
    "\n",
    "first_as_token_id = 3983\n",
    "second_as_token_id = 5686\n",
    "\n",
    "\n",
    "def discriminate_title_pair_with_llm(abstract,reference,prediction):\n",
    "    client: openai.OpenAI = openai.OpenAI()\n",
    "    correct_answer = random.choice([\"first\",\"second\"])\n",
    "    if correct_answer == \"first\":\n",
    "        first = reference\n",
    "        second = prediction\n",
    "        incorrect_answer = \"second\"\n",
    "    elif correct_answer == \"second\":\n",
    "        first = prediction\n",
    "        second = reference\n",
    "        incorrect_answer = \"first\"\n",
    "    \n",
    "\n",
    "    instruction = \"\"\"\n",
    "        You are given an abstract of a published real world news and a pair of possible titles. \n",
    "        One of the titles is the official title used by the news agency, which is generated \n",
    "        by a professional human. The other one is generated by a machine learning algorithm \n",
    "        and can be incorrect, incomplete, vague, or sound unprofessional. Your job is to identify \n",
    "        whether the first or the second title is the original human-generated title. \n",
    "        \n",
    "        Answer with a single word: \"first\" or \"second\", nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        Abstract: {abstract}\n",
    "        First title: {first}\n",
    "        Second title: {second}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        logit_bias={first_as_token_id: 100, second_as_token_id: 100},\n",
    "        max_tokens=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    model_completion = completion.choices[0].logprobs\n",
    "    json_response = json.loads(model_completion.json())\n",
    "    logprob = json_response[\"content\"][0][\"logprob\"]\n",
    "    token = json_response[\"content\"][0][\"token\"].lower()\n",
    "    probability = math.exp(logprob)\n",
    "\n",
    "\n",
    "    if token == correct_answer:\n",
    "        correct_flag = 1\n",
    "    elif token == incorrect_answer:\n",
    "        correct_flag = 0\n",
    "    else:\n",
    "        raise RuntimeError(f\"{json_response} is unexpected\")\n",
    "    \n",
    "    return correct_flag, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "def llm_as_discriminator(filepath, max_infer=100):\n",
    "    df = read_csv(filepath, encoding='cp1252')\n",
    "    total_correct = 0\n",
    "    probabilities = {\n",
    "        \"correct\": [],\n",
    "        \"incorrect\": []\n",
    "    }\n",
    "    for idx in range(max_infer):\n",
    "        item = df.iloc[idx]\n",
    "        abstract = item[\"Abstract\"]\n",
    "        reference = item[\"True Title\"]\n",
    "        prediction = item[\"Predicted Title\"]\n",
    "        correct_flag, probability = discriminate_title_pair_with_llm(abstract,reference,prediction)\n",
    "        total_correct += correct_flag\n",
    "        if correct_flag==1:\n",
    "            probabilities[\"correct\"].append(probability)\n",
    "        elif correct_flag==0:\n",
    "            probabilities[\"incorrect\"].append(probability)\n",
    "        else:\n",
    "            raise RuntimeError(f\"{correct_flag} is unexpected\")\n",
    "    print(f\"Discrimination accuracy: {100*total_correct/max_infer}\")\n",
    "    print(f\"Average confidence of correct discrimination: {np.mean(probabilities['correct'])}\")\n",
    "    print(f\"Average confidence of incorrect discrimination: {np.mean(probabilities['incorrect'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 46.0\n",
      "Average confidence of correct discrimination: 0.00243397403946814\n",
      "Average confidence of incorrect discrimination: 0.006831521486181354\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
