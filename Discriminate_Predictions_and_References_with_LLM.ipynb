{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular title prediction comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 14:29:28.003226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 14:29:28.459549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-24 14:29:29,909] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bashlab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_end(data):\n",
    "    words = data[0].split()\n",
    "    if words[0]==\"start\" and words[-1]==\"end\":\n",
    "        print(\"Removing start and end\")\n",
    "        old_data = data.copy()\n",
    "        data = []\n",
    "        for sample in old_data:\n",
    "            sample = \" \".join(sample.split()[1:-1])\n",
    "            data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(filepath, encoding='cp1252'):\n",
    "    df = read_csv(filepath, encoding=encoding)\n",
    "    references = remove_start_end(df[\"True Title\"])\n",
    "    predictions = remove_start_end(df[\"Predicted Title\"])\n",
    "    print(rouge.compute(predictions=predictions,references=references))\n",
    "    print(bleu.compute(predictions=predictions,references=[[ref] for ref in references]))\n",
    "    print(meteor.compute(predictions=predictions,references=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10182\n",
      "10182\n",
      "{'rouge1': 0.1366115222470731, 'rouge2': 0.04254364167446087, 'rougeL': 0.13179486331567025, 'rougeLsum': 0.1318208908662652}\n",
      "{'bleu': 0.036445868433343596, 'precisions': [0.1688180085410283, 0.06265792419638573, 0.034361192065431985, 0.03463917525773196], 'brevity_penalty': 0.6118439469006668, 'length_ratio': 0.6705657752107298, 'translation_length': 41447, 'reference_length': 61809}\n",
      "{'meteor': 0.09324901248878967}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9838\n",
      "9838\n",
      "{'rouge1': 0.15158954967586952, 'rouge2': 0.04889780780488909, 'rougeL': 0.14419852127153018, 'rougeLsum': 0.14434043878556196}\n",
      "{'bleu': 0.036447028298634054, 'precisions': [0.15408003706492152, 0.04847205807867635, 0.023869779118498573, 0.01753720971369368], 'brevity_penalty': 0.8667631546259996, 'length_ratio': 0.8748986623429267, 'translation_length': 69068, 'reference_length': 78944}\n",
      "{'meteor': 0.11078987532467596}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_attention_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1429085850744704, 'rouge2': 0.04106810454799074, 'rougeL': 0.13470933453489325, 'rougeLsum': 0.1348892319191301}\n",
      "{'bleu': 0.03166583165349626, 'precisions': [0.14564057307538505, 0.04218493030385285, 0.021559815689174585, 0.016730836541827093], 'brevity_penalty': 0.8207110764935666, 'length_ratio': 0.835014392630973, 'translation_length': 72521, 'reference_length': 86850}\n",
      "{'meteor': 0.09750387387678204}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_pointer_generator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1535061909525322, 'rouge2': 0.04740998192119289, 'rougeL': 0.14479962254003736, 'rougeLsum': 0.14479650082217893}\n",
      "{'bleu': 0.03548716771466826, 'precisions': [0.15497928703804645, 0.0461686792691408, 0.02268793739182783, 0.016938156034942187], 'brevity_penalty': 0.8714666574092829, 'length_ratio': 0.8790608528988979, 'translation_length': 73384, 'reference_length': 83480}\n",
      "{'meteor': 0.10596625515984237}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_vanilla_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.1609329257067545, 'rouge2': 0.03294434731374428, 'rougeL': 0.13704272906129478, 'rougeLsum': 0.13777026688182964}\n",
      "{'bleu': 0.01992006815185011, 'precisions': [0.13701067615658363, 0.02734375, 0.008658008658008658, 0.0048543689320388345], 'brevity_penalty': 1.0, 'length_ratio': 1.174503657262278, 'translation_length': 1124, 'reference_length': 957}\n",
      "{'meteor': 0.13661658349670927}\n"
     ]
    }
   ],
   "source": [
    "gpt_title_predictor_result_path = \"data/predicted_titles.csv\"\n",
    "compute_metrics(gpt_title_predictor_result_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitation of existing metrics\n",
    "\n",
    "As we can see, typical summary comparison metrics that rely on n-gram or other lexical similarities fail when the output is creative but valid. For example, our fine-tuned GPT often shows compelling and cohesive titles that are quite different from reference titles. Therefore, apart from Rouge-1 and METEOR, the metrics don't beat other models, although the results are significantly better upon human observation. To tackle this limitation, we propose a new metric: \"LLM Title Discriminator.\" It replaces human observation and tries to identify which title was originally used by the news agency. Lower Discriminator Accuracy indicates better titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Title Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Title Discriminator\n",
    "We'll use 20 randomly picked samples to fine-tuned GPT 3.5 to build our LLM Title Discriminator so that it can correctly identify samples that clearly appear to be machine-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import openai\n",
    "import random\n",
    "\n",
    "\n",
    "first_as_token_id = 3983\n",
    "second_as_token_id = 5686\n",
    "\n",
    "instruction = \"\"\"\n",
    "    You are given an abstract of a published real world news and a pair of possible titles. \n",
    "    One of the titles is the official title used by the news agency, which is generated \n",
    "    by a professional human. The other one is generated by a machine learning algorithm \n",
    "    and can be incorrect, incomplete, vague, or sound unprofessional or unnatural. \n",
    "    Your job is to identify whether the first or the second title \n",
    "    is the original human-generated title. \n",
    "    \n",
    "    Answer with a single word: \"first\" or \"second\", nothing else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result_files = [\n",
    "    \"LSTM_attention_stopwords.csv\",\n",
    "    \"LSTM_attention.csv\",\n",
    "    \"LSTM_pointer_generator.csv\",\n",
    "    \"LSTM_vanilla_data.csv\"\n",
    "]\n",
    "\n",
    "def generate_discriminator_training_data():\n",
    "    with open(\"discriminator_examples.jsonl\", \"w\") as f:\n",
    "        for filename in result_files:\n",
    "            df = read_csv(filename, encoding='cp1252')\n",
    "            df = df.sample(n=5)\n",
    "            \n",
    "            for item in df.iterrows():\n",
    "                item=item[1]\n",
    "                \n",
    "                abstract = item[\"Abstract\"]\n",
    "                reference = item[\"True Title\"]\n",
    "                prediction = item[\"Predicted Title\"]\n",
    "                \n",
    "                correct_answer = random.choice([\"first\",\"second\"])\n",
    "                if correct_answer == \"first\":\n",
    "                    first = reference\n",
    "                    second = prediction\n",
    "                    incorrect_answer = \"second\"\n",
    "                elif correct_answer == \"second\":\n",
    "                    first = prediction\n",
    "                    second = reference\n",
    "                    incorrect_answer = \"first\"\n",
    "                \n",
    "                prompt = f\"\"\"\n",
    "                    Abstract: {abstract}\n",
    "                    First title: {first}\n",
    "                    Second title: {second}\n",
    "                \"\"\"\n",
    "            \n",
    "                training_sample = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": instruction},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                        {\"role\": \"assistant\", \"content\": correct_answer}\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                training_json = json.dumps(training_sample)\n",
    "                f.write(training_json+\"\\n\")\n",
    "\n",
    "\n",
    "generate_discriminator_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "class OpenAIFineTuner:\n",
    "    \"\"\"\n",
    "    Class to fine tune OpenAI models\n",
    "    \"\"\"\n",
    "    def __init__(self, training_file_path, model_name, suffix):\n",
    "        self.training_file_path = training_file_path\n",
    "        self.model_name = model_name\n",
    "        self.suffix = suffix\n",
    "        self.file_object = None\n",
    "        self.fine_tuning_job = None\n",
    "        self.model_id = None\n",
    "\n",
    "    def create_openai_file(self):\n",
    "        self.file_object = client.files.create(\n",
    "            file=open(self.training_file_path, \"rb\"),\n",
    "            purpose=\"fine-tune\",\n",
    "        )\n",
    "\n",
    "    def wait_for_file_processing(self, sleep_time=20):\n",
    "        print(self.file_object.status)\n",
    "        while self.file_object.status != 'processed':\n",
    "            time.sleep(sleep_time)\n",
    "            print(\"File Status: \", self.file_object.status)\n",
    "\n",
    "    def create_fine_tuning_job(self):\n",
    "        self.fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "            training_file=self.file_object.id,\n",
    "            model=self.model_name,\n",
    "            suffix=self.suffix,\n",
    "        )\n",
    "\n",
    "    def wait_for_fine_tuning(self, sleep_time=45):\n",
    "        while client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).status != 'succeeded':\n",
    "            time.sleep(sleep_time)\n",
    "            print(\"Job Status: \", client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).status)\n",
    "\n",
    "    def retrieve_fine_tuned_model(self):\n",
    "        self.model_id = client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id).fine_tuned_model\n",
    "        return self.model_id\n",
    "\n",
    "    def fine_tune_model(self):\n",
    "        self.create_openai_file()\n",
    "        self.wait_for_file_processing()\n",
    "        self.create_fine_tuning_job()\n",
    "        self.wait_for_fine_tuning()\n",
    "        return self.retrieve_fine_tuned_model()\n",
    "\n",
    "fine_tuner = OpenAIFineTuner(\n",
    "    training_file_path=\"discriminator_examples.jsonl\",\n",
    "    model_name=\"gpt-3.5-turbo-1106\",\n",
    "    suffix=\"discriminator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  running\n",
      "Job Status:  succeeded\n",
      "ft:gpt-3.5-turbo-1106:worcester-polytechnic-institute:discriminator:9CX6Fawj\n"
     ]
    }
   ],
   "source": [
    "fine_tuner.fine_tune_model()\n",
    "model_id = fine_tuner.model_id\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_id = \"ft:gpt-3.5-turbo-1106:worcester-polytechnic-institute:discriminator:9CX6Fawj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Discriminator Metric\n",
    "Unless the reference and prediction are an exact match, the Title Discriminator model should attempt to identify which title is machine-generated. Higher Discriminator Accuracy indicates the weakness of titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminate_title_pair_with_llm(abstract,reference,prediction,model_id):\n",
    "    if reference.strip()==prediction.strip():\n",
    "        \"\"\"\n",
    "        For exact match, skip LLM discriminator\n",
    "        \"\"\"\n",
    "        return 0, 1.0\n",
    "    \n",
    "    correct_answer = random.choice([\"first\",\"second\"])\n",
    "    if correct_answer == \"first\":\n",
    "        first = reference\n",
    "        second = prediction\n",
    "        incorrect_answer = \"second\"\n",
    "    elif correct_answer == \"second\":\n",
    "        first = prediction\n",
    "        second = reference\n",
    "        incorrect_answer = \"first\"\n",
    "    \n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Abstract: {abstract}\n",
    "        First title: {first}\n",
    "        Second title: {second}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        logit_bias={first_as_token_id: 100, second_as_token_id: 100},\n",
    "        max_tokens=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    model_completion = completion.choices[0].logprobs\n",
    "    json_response = json.loads(model_completion.json())\n",
    "    logprob = json_response[\"content\"][0][\"logprob\"]\n",
    "    token = json_response[\"content\"][0][\"token\"].lower()\n",
    "    probability = math.exp(logprob)\n",
    "\n",
    "\n",
    "    if token == correct_answer:\n",
    "        correct_flag = 1\n",
    "    elif token == incorrect_answer:\n",
    "        correct_flag = 0\n",
    "    else:\n",
    "        raise RuntimeError(f\"{json_response} is unexpected\")\n",
    "    \n",
    "    return correct_flag, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def llm_as_discriminator(\n",
    "    filepath,\n",
    "    model_id,\n",
    "    max_infer=1000,\n",
    "    encoding='cp1252',\n",
    "    break_if_incorrect=False\n",
    "):\n",
    "    df = read_csv(filepath, encoding=encoding)\n",
    "    df = df.sample(n=max_infer)\n",
    "    total_correct = 0\n",
    "    probabilities = {\n",
    "        \"correct\": [],\n",
    "        \"incorrect\": []\n",
    "    }\n",
    "    for item in tqdm(df.iterrows()):\n",
    "        item = item[1]\n",
    "        abstract = item[\"Abstract\"]\n",
    "        reference = item[\"True Title\"]\n",
    "        if reference.split()[0]==\"start\" and reference.split()[-1]==\"end\":\n",
    "            reference = reference[1:-1]\n",
    "        prediction = item[\"Predicted Title\"]\n",
    "        if prediction.split()[0]==\"start\" and prediction.split()[-1]==\"end\":\n",
    "            prediction = prediction[1:-1]\n",
    "        correct_flag, probability = discriminate_title_pair_with_llm(\n",
    "            abstract,reference,prediction,model_id\n",
    "        )\n",
    "        total_correct += correct_flag\n",
    "        if correct_flag==1:\n",
    "            probabilities[\"correct\"].append(probability)\n",
    "        elif correct_flag==0:\n",
    "            probabilities[\"incorrect\"].append(probability)\n",
    "            if break_if_incorrect is True:\n",
    "                print(\n",
    "                    \"Confused example:\\n\"\n",
    "                    f\"Abstract: {abstract}\\n\"\n",
    "                    f\"Reference: {reference}\\n\"\n",
    "                    f\"Prediction: {prediction}\"\n",
    "                )\n",
    "                return\n",
    "        else:\n",
    "            raise RuntimeError(f\"{correct_flag} is unexpected\")\n",
    "    print(f\"Discrimination accuracy: {100*total_correct/max_infer}\")\n",
    "    print(f\"Average confidence of correct discrimination: {np.mean(probabilities['correct'])}\")\n",
    "    print(f\"Average confidence of incorrect discrimination: {np.mean(probabilities['incorrect'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Discriminator without Fine-tuning\n",
    "By default, GPT 3.5 isn't an expert title discriminator out of the box, unless it has been shown some examples of machine-generated titles. The following results show the weakness of zero-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_id = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [06:15,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 56.3\n",
      "Average confidence of correct discrimination: 0.2063006265721911\n",
      "Average confidence of incorrect discrimination: 0.1118991451035638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\", model_id=gpt35_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [05:50,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 56.9\n",
      "Average confidence of correct discrimination: 0.21359381651518153\n",
      "Average confidence of incorrect discrimination: 0.12883935566022092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention_stopwords.csv\", model_id=gpt35_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [05:44,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 60.3\n",
      "Average confidence of correct discrimination: 0.22049910418284183\n",
      "Average confidence of incorrect discrimination: 0.14561893402279072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_pointer_generator.csv\", model_id=gpt35_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [05:41,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 56.9\n",
      "Average confidence of correct discrimination: 0.20492478326689284\n",
      "Average confidence of incorrect discrimination: 0.14349475479734827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_vanilla_data.csv\", model_id=gpt35_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert LLM Title Discriminator Results\n",
    "Once fine-tuned on 20 samples, the LLM Title Discriminator model identifies machine-generated titles at an accuracy over 90% for our LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 92.9\n",
      "Average confidence of correct discrimination: 0.9999971073556101\n",
      "Average confidence of incorrect discrimination: 0.9994713166898006\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 96.9\n",
      "Average confidence of correct discrimination: 0.9999416524272656\n",
      "Average confidence of incorrect discrimination: 0.9714346741342155\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention_stopwords.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 98.1\n",
      "Average confidence of correct discrimination: 0.9999960779608249\n",
      "Average confidence of incorrect discrimination: 0.999126042695444\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_pointer_generator.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 97.9\n",
      "Average confidence of correct discrimination: 0.9999895465456974\n",
      "Average confidence of incorrect discrimination: 0.9993808599113149\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_vanilla_data.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our GPT-based Title Predictor\n",
    "The following cells show that fine-tuned LLMs can generate good quality titles at a much better rate. Discriminator Accuracy is only 72%, which is much lower than the LSTM models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:42,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 52.0\n",
      "Average confidence of correct discrimination: 0.3273687028695172\n",
      "Average confidence of incorrect discrimination: 0.3380004666446201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\n",
    "    gpt_title_predictor_result_path,\n",
    "    model_id=gpt35_id,\n",
    "    max_infer=100,\n",
    "    encoding='utf8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:36,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 73.0\n",
      "Average confidence of correct discrimination: 0.9997739809611879\n",
      "Average confidence of incorrect discrimination: 0.9999539307236794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\n",
    "    gpt_title_predictor_result_path,\n",
    "    model_id=finetuned_model_id,\n",
    "    max_infer=100,\n",
    "    encoding='utf8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of some \"good titles\"\n",
    "In the following cells, we'll see for which reference-prediction pairs the Title Discriminator failed. We'll see that vanilla GPT-3.5-Turbo often fails when the predicted title clearly reads wrong, incomplete, or unprofessional. On the other hand, when the expert / fine-tuned Title Discriminator fails, the titles are hard to distinguish quality-wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: report domestic intelligence agency listed 400 instances soldiers police officers intelligence officials suspected extremist actions posing “a significant danger ” \n",
      "Reference: right extremism taints german security services hundreds cases \n",
      "Prediction:  mexico police arrest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\", model_id=gpt35_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: top official in russia’s air force said the government was considering whether to base strategic bombers out of cuban territory or on venezuelan island \n",
      "Reference: russia is weighing latin bases general says \n",
      "Prediction:  russia and russia agree to discuss nato ties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention_stopwords.csv\", model_id=gpt35_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: lawsuit filed in march by <unk> <unk> <unk> card reader claims that her mother had <unk> love <unk> with the painter in the 1950s \n",
      "Reference: lawsuit in spain seeks recognition of another <unk> creation daughter \n",
      "Prediction:  the man who survived the death of the death\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_pointer_generator.csv\", model_id=gpt35_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: a court backed decision by the bangladeshi central bank to remove muhammad yunus the nobel laureate from bank the institution that he founded \n",
      "Reference:  removal of bank founder upheld \n",
      "Prediction:  pakistani court rules for killing of ex premier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_vanilla_data.csv\", model_id=gpt35_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:02,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: read latest updates mr christie \n",
      "Reference: chris christie climate change \n",
      "Prediction:  chris christie immigration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention.csv\", model_id=finetuned_model_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: prime minister yasuo fukuda of japan made the move in bid to raise his low approval ratings \n",
      "Reference: japanese premier reshuffles cabinet \n",
      "Prediction:  japan’s prime minister resigns over cabinet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_attention_stopwords.csv\", model_id=finetuned_model_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:02,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: the case of two families in siberia has stirred complaints about russia’s <unk> houses high volume operations where women are often treated <unk> \n",
      "Reference: russia stays transfixed by switch at birth \n",
      "Prediction:  russian court orders release of russian woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_pointer_generator.csv\", model_id=finetuned_model_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: with no to the fighting in yemen the united nations and other aid agencies said the humanitarian crisis there was worsening \n",
      "Reference:  yemeni army tries to oil fields as qaeda fighters advance \n",
      "Prediction:  u n says it will send aid to rebels in yemen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"LSTM_vanilla_data.csv\", model_id=finetuned_model_id, break_if_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confused example:\n",
      "Abstract: A giant lobster named Larry is at the center of a small town’s identity. What happens if he leaves? And what does a dispute about its future say about the culture of Australia’s “Big Things”?\n",
      "Reference: For Sale: 55-Foot-Tall Lobster. Owners in a Pinch. Can You Help?\n",
      "Prediction: Who Owns Larry? Debating the Glories of Australia’s ‘Big Things’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\n",
    "    gpt_title_predictor_result_path,\n",
    "    model_id=finetuned_model_id,\n",
    "    break_if_incorrect=True,\n",
    "    encoding='utf8',\n",
    "    max_infer=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last example shown above reveals why we need such a Title Discriminator metric instead of relying of Rouge or BLEU or other such metrics. The LLM-predicted title is quite good. It's completely different from the reference title, but it's compelling and conveys the point of the article well. Although this sample would generate a very bad Rouge or BLEU score, it should be considered an excellent predicted sample, which our LLM Title Discriminator confirms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.08348140517989147, 'rouge2': 0.014349147942793569, 'rougeL': 0.08001952359935977, 'rougeLsum': 0.0799971308931128}\n",
      "{'bleu': 0.0, 'precisions': [0.06989139179404666, 0.011364954192276469, 0.0002738975623116954, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0049519959575544, 'translation_length': 9944, 'reference_length': 9895}\n",
      "{'meteor': 0.048710093053581115}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_attention_stopwords_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [04:34,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 99.7\n",
      "Average confidence of correct discrimination: 0.9999991512053142\n",
      "Average confidence of incorrect discrimination: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"lstm/LSTM_attention_stopwords_10k.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.06097515982152964, 'rouge2': 0.010941300818169103, 'rougeL': 0.059457546833253744, 'rougeLsum': 0.059268668747649134}\n",
      "{'bleu': 0.0, 'precisions': [0.06699958385351644, 0.011427596793450453, 0.0006641576267434137, 0.0], 'brevity_penalty': 0.6681477531377876, 'length_ratio': 0.7126334519572953, 'translation_length': 7209, 'reference_length': 10116}\n",
      "{'meteor': 0.03325435038898777}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"lstm/LSTM_attention_stopwords_synthetic20k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [04:56,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 98.9\n",
      "Average confidence of correct discrimination: 0.9999496258726065\n",
      "Average confidence of incorrect discrimination: 0.9770476898962854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\"lstm/LSTM_attention_stopwords_synthetic20k.csv\", model_id=finetuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing start and end\n",
      "Removing start and end\n",
      "{'rouge1': 0.17377642099043022, 'rouge2': 0.04582447776681522, 'rougeL': 0.1632700850061531, 'rougeLsum': 0.16395885096155988}\n",
      "{'bleu': 0.027436297784275653, 'precisions': [0.18723404255319148, 0.0512396694214876, 0.015841584158415842, 0.007407407407407408], 'brevity_penalty': 0.8422897475758973, 'length_ratio': 0.8535108958837773, 'translation_length': 705, 'reference_length': 826}\n",
      "{'meteor': 0.1190630275488683}\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\"word2vec_output.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:23,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 95.0\n",
      "Average confidence of correct discrimination: 0.9999997597620491\n",
      "Average confidence of incorrect discrimination: 0.9999997169473731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\n",
    "    \"word2vec_output.csv\",\n",
    "    model_id=finetuned_model_id,\n",
    "    encoding='utf8',\n",
    "    max_infer=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:39,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination accuracy: 51.0\n",
      "Average confidence of correct discrimination: 0.15412516972964677\n",
      "Average confidence of incorrect discrimination: 0.12357379883057334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_as_discriminator(\n",
    "    \"word2vec_output.csv\",\n",
    "    model_id=gpt35_id,\n",
    "    encoding='utf8',\n",
    "    max_infer=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
