{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/NYT_dataset.csv')\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(df['abstract'].to_numpy(), df['title'].to_numpy(), test_size=0.1, random_state=26, shuffle=True)\n",
    "\n",
    "system_prompt: str = \"Given the abstract, generate the most likely New York Times title.\"\n",
    "\n",
    "\n",
    "def create_jsonl_file(data: list[tuple[str, str]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates or appends to a .jsonl file with the given data. If the file does not exist, it is created.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A list of tuples containing pairs of abstracts and titles.\n",
    "    - filename: The name of the file to be created or appended to.\n",
    "    \"\"\"\n",
    "    file_path = f'./data/{filename}.jsonl'\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        for abstract, title in data:\n",
    "            if pd.isna(abstract) or pd.isna(title):\n",
    "                continue\n",
    "            entry = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": abstract},\n",
    "                    {\"role\": \"assistant\", \"content\": title}\n",
    "                ]\n",
    "            }\n",
    "            file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Splitting the dataset into training and testing datasets\n",
    "train_data = list(zip(x_tr, y_tr))\n",
    "test_data = list(zip(x_val, y_val))\n",
    "\n",
    "# Generating or updating the train.jsonl and test.jsonl files\n",
    "create_jsonl_file(train_data, 'train')\n",
    "create_jsonl_file(test_data, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_jsonl_file(data: list[tuple[str, str]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates or appends to a synthetic .jsonl file with the given data in a synthetic manner. \n",
    "    If the file does not exist, it is created.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A list of tuples containing pairs of abstracts and titles.\n",
    "    - filename: The name of the file to be created or appended to, within the synthetic directory.\n",
    "    \"\"\"\n",
    "    synthetic_file_path = f'./synthetic/{filename}.jsonl'\n",
    "    with open(synthetic_file_path, 'a', encoding='utf-8') as file:\n",
    "        for abstract, title in data:\n",
    "            synthetic_entry = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": abstract},\n",
    "                    {\"role\": \"assistant\", \"content\": title}\n",
    "                ]\n",
    "            }\n",
    "            file.write(json.dumps(synthetic_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Load synthetic dataset from .jsonl\n",
    "def load_synthetic_data(file_path: str) -> list[tuple[str, str]]:\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            abstract = entry['abstract']\n",
    "            title = entry['title']\n",
    "            data.append((abstract, title))\n",
    "    return data\n",
    "\n",
    "synthetic_data = load_synthetic_data('./synthetic/wikinews_synthetic_data.jsonl')\n",
    "\n",
    "# Splitting the synthetic dataset into training dataset only\n",
    "# Assuming the entire dataset is for training as per the original code's test_size=0.0\n",
    "synthetic_train_data = synthetic_data\n",
    "\n",
    "# Generating or updating the synthetic train.jsonl file\n",
    "create_synthetic_jsonl_file(synthetic_train_data, 'synthetic_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_limited_jsonl_file(source_file_path: str, target_file_path: str, limit: int = 1000) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new .jsonl file from the first 'limit' entries of an existing .jsonl file.\n",
    "\n",
    "    Parameters:\n",
    "    - source_file_path: The path to the source .jsonl file.\n",
    "    - target_file_path: The path where the new limited .jsonl file will be created.\n",
    "    - limit: The maximum number of entries to include in the new file.\n",
    "    \"\"\"\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as source_file, \\\n",
    "         open(target_file_path, 'w', encoding='utf-8') as target_file:\n",
    "        for i, line in enumerate(source_file):\n",
    "            if i < limit:\n",
    "                target_file.write(line)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# Creating a limited version of the synthetic train.jsonl file with only the first 1000 rows\n",
    "create_limited_jsonl_file('./data/train.jsonl', './data/train_limited.jsonl', 1000)\n",
    "\n",
    "# Creating a limited version of the synthetic test.jsonl file with only the first 1000 rows\n",
    "create_limited_jsonl_file('./data/test.jsonl', './data/test_limited.jsonl', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5522143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_total_tokens(file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the total number of tokens for all messages in a .jsonl file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path to the .jsonl file containing the messages.\n",
    "\n",
    "    Returns:\n",
    "    - The total number of tokens for all messages in the file.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            messages = entry[\"messages\"]\n",
    "            for message in messages:\n",
    "                for key, value in message.items():\n",
    "                    if isinstance(value, str):\n",
    "                        total_tokens += len(encoding.encode(value))\n",
    "\n",
    "    return total_tokens\n",
    "\n",
    "calculate_total_tokens('./data/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a csv file for 1000 examples with the fine-tuned GPT-3.5 model with the headers 'Model', 'Abstract', 'True Title', and 'Predicted Title'\n",
    "# ft:gpt-3.5-turbo-0125:personal:nyt-dataset:98g0EH1s\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "def generate_titles(model: str, dataset_path: str, output_csv_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates predicted titles for all abstracts in the dataset using the specified fine-tuned GPT-3.5 model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model identifier for the fine-tuned GPT-3.5 model.\n",
    "    - dataset_path: The path to the dataset file containing abstracts.\n",
    "    - output_csv_path: The path where the output CSV file will be saved.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as dataset_file, open(output_csv_path, 'a+', newline='', encoding='utf-8') as output_file:\n",
    "        output_file.seek(0)  # Move to the start of the file before reading\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow(['Model', 'Abstract', 'True Title', 'Predicted Title'])\n",
    "\n",
    "        for line in dataset_file:\n",
    "            entry = json.loads(line)\n",
    "            messages = entry[\"messages\"]\n",
    "            system_message = \"\"\n",
    "            user_message = \"\"\n",
    "            true_title = \"\"\n",
    "            for message in messages:\n",
    "                if message[\"role\"] == \"system\":\n",
    "                    system_message = message[\"content\"]\n",
    "                elif message[\"role\"] == \"user\":\n",
    "                    user_message = message[\"content\"]\n",
    "                elif message[\"role\"] == \"assistant\":\n",
    "                    true_title = message[\"content\"]\n",
    "\n",
    "            prompt = system_message + \"\\n\" + user_message\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ]\n",
    "            )\n",
    "            predicted_title = response.choices[0].message.content\n",
    "            predicted_title = predicted_title.replace('\\n', ' ')\n",
    "            csv_writer.writerow([model, user_message, true_title, predicted_title])\n",
    "\n",
    "generate_titles('ft:gpt-3.5-turbo-0125:personal:nyt-dataset:98g0EH1s', './data/test_limited.jsonl', './data/predicted_titles.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
